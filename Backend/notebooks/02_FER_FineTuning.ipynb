{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fb08f8-e2c8-4823-abe9-9f212e9a3501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm # For nice progress bars\n",
    "\n",
    "\n",
    "print(\"All libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb10e61c-dc1c-4118-af76-552dfddc0d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Transforms defined.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:30: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup Device ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 2. Define Data Paths ---\n",
    "DATA_DIR = '../data/MELD_processed/faces/'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "DEV_DIR = os.path.join(DATA_DIR, 'dev') # We'll use the 'dev' set for validation\n",
    "\n",
    "# --- 3. Define Image Transforms ---\n",
    "# Use the feature extractor's recommended settings for normalization\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "image_mean = feature_extractor.image_mean\n",
    "image_std = feature_extractor.image_std\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((feature_extractor.size['height'], feature_extractor.size['width'])),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((feature_extractor.size['height'], feature_extractor.size['width'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=image_mean, std=image_std),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a123618-38df-4b31-aeed-402ab7fafa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking contents of: ../data/MELD_processed/faces/\n",
      "['dev', 'test', 'train']\n",
      "\n",
      "Checking contents of: ../data/MELD_processed/faces/train\n",
      "['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Let's list the contents of the 'faces' and 'faces/train' directories\n",
    "DATA_DIR = '../data/MELD_processed/faces/'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "\n",
    "print(f\"Checking contents of: {DATA_DIR}\")\n",
    "print(os.listdir(DATA_DIR))\n",
    "\n",
    "print(f\"\\nChecking contents of: {TRAIN_DIR}\")\n",
    "print(os.listdir(TRAIN_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8fefcf-9187-4133-9577-7123390a6969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets using ImageFolder...\n",
      "Training data size: 2571\n",
      "Validation data size: 1256\n",
      "Found 7 classes: ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Create Datasets ---\n",
    "print(\"Loading datasets using ImageFolder...\")\n",
    "image_datasets = {\n",
    "    'train': datasets.ImageFolder(TRAIN_DIR, data_transforms['train']),\n",
    "    'val': datasets.ImageFolder(DEV_DIR, data_transforms['val'])\n",
    "}\n",
    "\n",
    "# --- 5. Create DataLoaders ---\n",
    "# DataLoaders turn our dataset into batches to feed to the GPU\n",
    "dataloaders = {\n",
    "    'train': DataLoader(image_datasets['train'], batch_size=32, shuffle=True, num_workers=0), # <-- SET TO 0\n",
    "    'val': DataLoader(image_datasets['val'], batch_size=32, shuffle=False, num_workers=0)  # <-- SET TO 0\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "print(f\"Training data size: {dataset_sizes['train']}\")\n",
    "print(f\"Validation data size: {dataset_sizes['val']}\")\n",
    "print(f\"Found {len(class_names)} classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd5cfc6-701c-48cc-a872-fa2b5209d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model labels: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Our dataset labels:    ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise'] (from ImageFolder)\n",
      "\n",
      "Updated model label2id: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "\n",
      "Model loaded and moved to GPU.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Load Pretrained Model ---\n",
    "model_name = \"trpakov/vit-face-expression\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "# --- 7. Map MELD Labels to Model Labels ---\n",
    "# Let's check the model's original labels\n",
    "model_labels = list(model.config.id2label.values())\n",
    "print(f\"Original model labels: {model_labels}\")\n",
    "print(f\"Our dataset labels:    {class_names} (from ImageFolder)\")\n",
    "\n",
    "# Create a mapping from our dataset's integer index to the model's integer index\n",
    "# This is a bit complex, but ensures 'anger' in our data maps to 'angry' in the model\n",
    "model_label2id = model.config.label2id\n",
    "# Our dataset's labels are sorted alphabetically by ImageFolder\n",
    "# class_names = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "# Model's labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# Let's create a remap dictionary:\n",
    "remap = {\n",
    "    'anger': 'angry',\n",
    "    'disgust': 'disgust',\n",
    "    'fear': 'fear',\n",
    "    'joy': 'happy',  # This is the only name change\n",
    "    'neutral': 'neutral',\n",
    "    'sadness': 'sad',\n",
    "    'surprise': 'surprise'\n",
    "}\n",
    "\n",
    "# Now, create the new label maps for the model config\n",
    "# This ensures the model's output neurons match our folder names\n",
    "model.config.label2id = {remap[label]: i for i, label in enumerate(class_names)}\n",
    "model.config.id2label = {i: remap[label] for i, label in enumerate(class_names)}\n",
    "\n",
    "print(f\"\\nUpdated model label2id: {model.config.label2id}\")\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(\"\\nModel loaded and moved to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67c8c6a9-39c9-43b0-b543-fcb4fddc6966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer and Loss Function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Define Optimizer and Loss ---\n",
    "# We'll only fine-tune the final layer (classifier) for speed,\n",
    "# but for better performance, we can train all parameters.\n",
    "# Let's train all parameters.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2) # <-- ADDED weight_decay\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Optimizer and Loss Function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baca8a05-3740-4bfd-aeb6-95197cbc9650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model fine-tuning...\n",
      "\n",
      "--- Epoch 1/4 ---\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065082c25b9a4a768a3c8893cbc44f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.5483 Acc: 0.4395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6869205668846e386a83b6a41391e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.1345 Acc: 0.1815\n",
      "New best val acc: 0.1815!\n",
      "\n",
      "--- Epoch 2/4 ---\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8df9e95ebf4e519e0ff97e14db3446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.3208 Acc: 0.5294\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bc502072614a8db2e0f4e97d1c90e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2372 Acc: 0.1704\n",
      "\n",
      "--- Epoch 3/4 ---\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1507cbcc3cf46138082b3398859ae92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.0992 Acc: 0.6352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b172ced9973a4d39bb0c33f9611c3d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2923 Acc: 0.1752\n",
      "\n",
      "--- Epoch 4/4 ---\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fff16a66cc40cf8c6be08845bafe91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8913 Acc: 0.7165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127eca0fd2e34a10b3367e79bfd42cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing val batches:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 2.2632 Acc: 0.1847\n",
      "New best val acc: 0.1847!\n",
      "\n",
      "Training complete in 30m 35s\n",
      "Best val Acc: 0.184713\n",
      "\n",
      "--- Training Finished ---\n",
      "Best model saved to ../models/fer_model_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "#------- 9: The Training & Validation Loop    -------#\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    since = time.time()\n",
    "    \n",
    "    # To save the best model weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\n--- Epoch {epoch+1}/{num_epochs} ---')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for batch in tqdm(dataloaders[phase], desc=f\"Processing {phase} batches\"):\n",
    "                inputs = batch[0].to(device) # Get images\n",
    "                labels = batch[1].to(device) # Get labels\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs\n",
    "                    # The model returns a dict, we need the 'logits'\n",
    "                    outputs = model(inputs)\n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    _, preds = torch.max(logits, 1)\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best one\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print(f'New best val acc: {best_acc:.4f}!')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# --- 8. START TRAINING ---\n",
    "print(\"Starting model fine-tuning...\")\n",
    "\n",
    "# Run the training function\n",
    "model_ft = train_model(model, criterion, optimizer, num_epochs=4) # <-- CHANGED 10 to 4\n",
    "\n",
    "# --- 9. Save the Best Model ---\n",
    "# After training, save your fine-tuned model\n",
    "SAVE_PATH = '../models/fer_model_finetuned.pth'\n",
    "\n",
    "# --- ADD THESE TWO LINES ---\n",
    "import os\n",
    "os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n",
    "# ---------------------------\n",
    "\n",
    "torch.save(model_ft.state_dict(), SAVE_PATH)\n",
    "\n",
    "print(f\"\\n--- Training Finished ---\")\n",
    "print(f\"Best model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fdad8-c545-4711-8b0c-fc838f9e0c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trisense V2)",
   "language": "python",
   "name": "trisense_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
