{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f967737-c3fe-4ebe-893b-f84d7d6a9d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sharm\\.conda\\envs\\trisense\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4880b197-9246-408b-bf0a-9ffa43099071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.3\n"
     ]
    }
   ],
   "source": [
    "import moviepy\n",
    "print(moviepy.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c456ec-8d48-4275-846f-c04b0b77dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete and directories created.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2  # This is opencv-python\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from moviepy.editor import VideoFileClip\n",
    "from tqdm.auto import tqdm  # A nice progress bar!\n",
    "\n",
    "# Set up our file paths\n",
    "RAW_DATA_PATH = '../data/MELD_raw/'\n",
    "PROC_DATA_PATH = '../data/MELD_processed/'\n",
    "\n",
    "# Create the processed data directories if they don't exist\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'audio', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'audio', 'dev'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'audio', 'test'), exist_ok=True)\n",
    "\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'video_frames', 'train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'video_frames', 'dev'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROC_DATA_PATH, 'video_frames', 'test'), exist_ok=True)\n",
    "\n",
    "print(\"Imports complete and directories created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "716af267-1e5d-4650-a6c0-29f5d0b4340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded all CSVs.\n",
      "Training data shape: (9989, 11)\n",
      "Dev data shape: (1109, 11)\n",
      "Test data shape: (2610, 11)\n"
     ]
    }
   ],
   "source": [
    "# Define the base path to the MELD.Raw folder\n",
    "# (We assume MELD.Raw is inside your data/MELD_raw/ folder)\n",
    "BASE_MELD_PATH = os.path.join(RAW_DATA_PATH, 'MELD.Raw')\n",
    "\n",
    "# Load the CSVs from their different locations\n",
    "try:\n",
    "    df_train = pd.read_csv(os.path.join(BASE_MELD_PATH, 'train', 'train_sent_emo.csv'))\n",
    "    df_dev = pd.read_csv(os.path.join(BASE_MELD_PATH, 'dev_sent_emo.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(BASE_MELD_PATH, 'test_sent_emo.csv'))\n",
    "    \n",
    "    print(\"Successfully loaded all CSVs.\")\n",
    "    print(\"Training data shape:\", df_train.shape)\n",
    "    print(\"Dev data shape:\", df_dev.shape)\n",
    "    print(\"Test data shape:\", df_test.shape)\n",
    "    \n",
    "    df_train.head()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\n! Please make sure your 'MELD.Raw' folder is inside 'data/MELD_raw/'\")\n",
    "    print(\"Your full path should look like: trisense_project/data/MELD_raw/MELD.Raw/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f818431a-44aa-49a7-adea-c43c3b90a0fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mtcnn in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: joblib>=1.4.2 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from mtcnn) (1.5.2)\n",
      "Requirement already satisfied: lz4>=4.3.3 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from mtcnn) (4.4.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (6.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (2.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (9.4.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sharm\\.conda\\envs\\trisense\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mtcnn tensorflow --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9965201c-0137-4768-b9d5-ad8b88ab7ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MTCNN imported and initialized correctly!\n",
      "Face detection test completed. Faces found: 0\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "\n",
    "detector = MTCNN()\n",
    "print(\"✅ MTCNN imported and initialized correctly!\")\n",
    "\n",
    "# quick sanity check: run a dummy face detection on an empty image\n",
    "import numpy as np\n",
    "dummy_img = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "faces = detector.detect_faces(dummy_img)\n",
    "print(\"Face detection test completed. Faces found:\", len(faces))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b2f0d1-de80-4cb5-ad9f-3517d4278693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train set... searching for videos in: ../data/MELD_raw/MELD.Raw\\train\\train_splits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee85d4734f0344118861eeb71c2eb73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train set:   0%|          | 0/9989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dev set... searching for videos in: ../data/MELD_raw/MELD.Raw\\dev\\dev_splits_complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685bddb661644f798ee7538e3a9d58eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing dev set:   0%|          | 0/1109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test set... searching for videos in: ../data/MELD_raw/MELD.Raw\\test\\output_repeated_splits_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7238446475cf4b9fb3a48e7fae78cdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test set:   0%|          | 0/2610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FER face extraction complete! Check '../data/MMELD_processed/faces/' for results.\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- This is the dictionary we used in the audio-processing cell ---\n",
    "split_to_video_folder = {\n",
    "    'train': 'train_splits',\n",
    "    'dev': 'dev_splits_complete',\n",
    "    'test': 'output_repeated_splits_test'\n",
    "}\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Initialize MTCNN face detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Directory for processed faces\n",
    "FACE_SAVE_PATH = os.path.join(PROC_DATA_PATH, 'faces')\n",
    "os.makedirs(FACE_SAVE_PATH, exist_ok=True)\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    os.makedirs(os.path.join(FACE_SAVE_PATH, split), exist_ok=True)\n",
    "\n",
    "# --- OPTIMIZED FUNCTION ---\n",
    "def crop_faces_from_video(video_path, output_dir):\n",
    "    \"\"\"\n",
    "    Extract faces from a video using MTCNN and save cropped face images.\n",
    "    OPTIMIZED: Only processes 2 frames per second.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_num = 0\n",
    "        success, frame = cap.read()\n",
    "        \n",
    "        # --- NEW: Get video FPS to calculate skip rate ---\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        if fps == 0 or fps is None:\n",
    "            fps = 30  # Default to 30 if FPS is not readable\n",
    "        \n",
    "        # Process 2 frames per second\n",
    "        frame_skip = int(fps / 2)\n",
    "        if frame_skip == 0:\n",
    "            frame_skip = 1  # Ensure we skip at least 1, not 0\n",
    "        # ------------------------------------------------\n",
    "        \n",
    "        while success:\n",
    "            # --- NEW: Check if this is a frame we should process ---\n",
    "            if frame_num % frame_skip == 0:\n",
    "                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                detections = detector.detect_faces(rgb)\n",
    "                \n",
    "                for i, det in enumerate(detections):\n",
    "                    if det['confidence'] < 0.9:\n",
    "                        continue\n",
    "                    x, y, w, h = det['box']\n",
    "                    x, y = max(0, x), max(0, y)\n",
    "                    face = rgb[y:y+h, x:x+w]\n",
    "                    \n",
    "                    face_img = Image.fromarray(face).resize((224, 224))\n",
    "                    save_name = f\"frame_{frame_num:04d}_face{i}.jpg\" # Use frame_num for unique names\n",
    "                    face_img.save(os.path.join(output_dir, save_name))\n",
    "            \n",
    "            # ------------------------------------------------\n",
    "            \n",
    "            frame_num += 1\n",
    "            success, frame = cap.read()\n",
    "            \n",
    "        cap.release()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "# --- END OPTIMIZED FUNCTION ---\n",
    "\n",
    "def process_split_faces(df, split='train'):\n",
    "    \"\"\"\n",
    "    Process all MELD videos in a given split to extract faces organized by emotion.\n",
    "    \"\"\"\n",
    "    # Get the correct sub-folder name for the videos\n",
    "    video_subfolder = split_to_video_folder[split]\n",
    "    split_video_path = os.path.join(BASE_MELD_PATH, split, video_subfolder)\n",
    "    \n",
    "    split_face_path = os.path.join(FACE_SAVE_PATH, split)\n",
    "    \n",
    "    print(f\"Processing {split} set... searching for videos in: {split_video_path}\")\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing {split} set\"):\n",
    "        emotion = row['Emotion']\n",
    "        video_filename = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.mp4\"\n",
    "        \n",
    "        video_path = os.path.join(split_video_path, video_filename)\n",
    "        \n",
    "        if not os.path.exists(video_path):\n",
    "            continue\n",
    "        \n",
    "        emotion_dir = os.path.join(split_face_path, emotion)\n",
    "        os.makedirs(emotion_dir, exist_ok=True)\n",
    "        \n",
    "        # Call the face extraction function\n",
    "        crop_faces_from_video(video_path, emotion_dir)\n",
    "\n",
    "\n",
    "# --- Run face extraction ---\n",
    "# (Make sure Cells 3 and 4 have been run to define df_train, etc.)\n",
    "process_split_faces(df_train, 'train')\n",
    "process_split_faces(df_dev, 'dev')\n",
    "process_split_faces(df_test, 'test')\n",
    "\n",
    "print(\"✅ FER face extraction complete! Check '../data/MMELD_processed/faces/' for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3119950-7b57-4353-b982-d4a8bbf6d7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Audio (SER) and Text (TER) processing...\n",
      "Processing train data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▊                                    | 1165/9989 [05:20<47:11,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error processing dia125_utt3: MoviePy error: failed to read the duration of file ../data/MELD_raw/MELD.Raw\\train\\train_splits\\dia125_utt3.mp4.\n",
      "Here are the file infos returned by ffmpeg:\n",
      "\n",
      "ffmpeg version 7.1-essentials_build-www.gyan.dev Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 14.2.0 (Rev1, Built by MSYS2 project)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-w32threads --disable-autodetect --enable-fontconfig --enable-iconv --enable-gnutls --enable-libxml2 --enable-gmp --enable-bzlib --enable-lzma --enable-zlib --enable-libsrt --enable-libssh --enable-libzmq --enable-avisynth --enable-sdl2 --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxvid --enable-libaom --enable-libopenjpeg --enable-libvpx --enable-mediafoundation --enable-libass --enable-libfreetype --enable-libfribidi --enable-libharfbuzz --enable-libvidstab --enable-libvmaf --enable-libzimg --enable-amf --enable-cuda-llvm --enable-cuvid --enable-dxva2 --enable-d3d11va --enable-d3d12va --enable-ffnvcodec --enable-libvpl --enable-nvdec --enable-nvenc --enable-vaapi --enable-libgme --enable-libopenmpt --enable-libopencore-amrwb --enable-libmp3lame --enable-libtheora --enable-libvo-amrwbenc --enable-libgsm --enable-libopencore-amrnb --enable-libopus --enable-libspeex --enable-libvorbis --enable-librubberband\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 000001e0904bf600] Format mov,mp4,m4a,3gp,3g2,mj2 detected only with low score of 1, misdetection possible!\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 000001e0904bf600] moov atom not found\n",
      "[in#0 @ 000001e0904bf240] Error opening input: Invalid data found when processing input\n",
      "Error opening input file ../data/MELD_raw/MELD.Raw\\train\\train_splits\\dia125_utt3.mp4.\n",
      "Error opening input files: Invalid data found when processing input\n",
      ". Skipping file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 9989/9989 [41:49<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing train. Text data saved to ../data/MELD_processed/train_text.csv\n",
      "Processing dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████ | 1084/1109 [04:24<00:06,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Error processing dia110_utt7: MoviePy error: the file ../data/MELD_raw/MELD.Raw\\dev\\dev_splits_complete\\dia110_utt7.mp4 could not be found!\n",
      "Please check that you entered the correct path.. Skipping file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 1109/1109 [04:30<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing dev. Text data saved to ../data/MELD_processed/dev_text.csv\n",
      "Processing test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 2610/2610 [10:28<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing test. Text data saved to ../data/MELD_processed/test_text.csv\n",
      "\n",
      "--- All Preprocessing Complete! ---\n",
      "Your datasets are ready in '../data/MELD_processed/'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# We need to re-define these paths in case the kernel was restarted\n",
    "RAW_DATA_PATH = '../data/MELD_raw/'\n",
    "PROC_DATA_PATH = '../data/MELD_processed/'\n",
    "\n",
    "# We also need the base MELD path and the split mapping again\n",
    "BASE_MELD_PATH = os.path.join(RAW_DATA_PATH, 'MELD.Raw')\n",
    "split_to_video_folder = {\n",
    "    'train': 'train_splits',\n",
    "    'dev': 'dev_splits_complete',\n",
    "    'test': 'output_repeated_splits_test'\n",
    "}\n",
    "\n",
    "print(\"Starting Audio (SER) and Text (TER) processing...\")\n",
    "\n",
    "def process_audio_and_text(df, data_split):\n",
    "    \"\"\"\n",
    "    This function processes one data split (train, dev, or test).\n",
    "    \n",
    "    For each row, it:\n",
    "    1. Extracts audio, resamples to 16kHz, and saves as .wav\n",
    "    2. Collects text, emotion, and file paths into a list\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Processing {data_split} data...\")\n",
    "    \n",
    "    # Get the correct sub-folder name for the videos\n",
    "    video_subfolder = split_to_video_folder[data_split]\n",
    "    \n",
    "    # We will also collect the text data into a list\n",
    "    text_data_list = []\n",
    "    \n",
    "    # Use tqdm to show a progress bar\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        try:\n",
    "            # 1. Define file names and paths\n",
    "            dialogue_id = row['Dialogue_ID']\n",
    "            utterance_id = row['Utterance_ID']\n",
    "            file_name_base = f\"dia{dialogue_id}_utt{utterance_id}\"\n",
    "            \n",
    "            # Raw video file path\n",
    "            raw_video_path = os.path.join(BASE_MELD_PATH, data_split, video_subfolder, f\"{file_name_base}.mp4\")\n",
    "\n",
    "            # --- 1. Audio Preprocessing (SER) ---\n",
    "            audio_save_path = os.path.join(PROC_DATA_PATH, 'audio', data_split, f\"{file_name_base}.wav\")\n",
    "            \n",
    "            # Use moviepy to extract audio\n",
    "            with VideoFileClip(raw_video_path) as video_clip:\n",
    "                audio_clip = video_clip.audio\n",
    "                temp_audio_path = os.path.join(PROC_DATA_PATH, 'temp_audio.wav')\n",
    "                audio_clip.write_audiofile(temp_audio_path, logger=None, verbose=False)\n",
    "            \n",
    "            # Load with librosa to resample to 16kHz (for Wav2Vec2)\n",
    "            y, sr = librosa.load(temp_audio_path, sr=16000)\n",
    "            \n",
    "            # Save the resampled audio\n",
    "            sf.write(audio_save_path, y, sr)\n",
    "            \n",
    "            # --- 2. Text Preprocessing (TER) ---\n",
    "            text_data_list.append({\n",
    "                'text': row['Utterance'],\n",
    "                'emotion': row['Emotion'],\n",
    "                'audio_path': audio_save_path, # We save the new audio path\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            # We'll skip errors, but print them to know\n",
    "            print(f\"Warning: Error processing {file_name_base}: {e}. Skipping file.\")\n",
    "            \n",
    "    # Clean up the temporary audio file\n",
    "    if os.path.exists(os.path.join(PROC_DATA_PATH, 'temp_audio.wav')):\n",
    "        os.remove(os.path.join(PROC_DATA_PATH, 'temp_audio.wav'))\n",
    "        \n",
    "    # Now, save the collected text data as a new, clean CSV\n",
    "    text_df = pd.DataFrame(text_data_list)\n",
    "    text_csv_path = os.path.join(PROC_DATA_PATH, f'{data_split}_text.csv')\n",
    "    text_df.to_csv(text_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Finished processing {data_split}. Text data saved to {text_csv_path}\")\n",
    "    return text_df\n",
    "\n",
    "# --- Run the Processing ---\n",
    "# We use the DataFrames (df_train, df_dev, df_test) that are still in memory from Cell 2\n",
    "# If you get a 'NameError', just re-run Cell 2 first.\n",
    "\n",
    "df_train_proc = process_audio_and_text(df_train, 'train')\n",
    "df_dev_proc = process_audio_and_text(df_dev, 'dev')\n",
    "df_test_proc = process_audio_and_text(df_test, 'test')\n",
    "\n",
    "print(\"\\n--- All Preprocessing Complete! ---\")\n",
    "print(\"Your datasets are ready in '../data/MELD_processed/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8435e0-c865-408d-bc0b-aa1a806c5873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Trisense V2)",
   "language": "python",
   "name": "trisense_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
